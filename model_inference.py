import hashlib
import psutil
import threading
from queue import PriorityQueue
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor
import resource

class ProductionInferenceOptimizer:
    def __init__(self):
        self.cpu_count = mp.cpu_count()
        self.memory_limit = self.get_memory_limit()
        self.inference_queue = PriorityQueue(maxsize=50000)
        self.batch_processor = None
        self.performance_stats = {
            'inferences_per_second': 0,
            'avg_batch_size': 0,
            'memory_usage_mb': 0,
            'cpu_usage_percent': 0,
            'cache_efficiency': 0
        }
        
    def get_memory_limit(self):
        """Get memory limit for inference caching"""
        try:
            available_memory = psutil.virtual_memory().available
            # Use 30% of available memory for model inference
            return int(available_memory * 0.3)
        except:
            return 2 * 1024 * 1024 * 1024  # 2GB default
    
    def optimize_batch_size(self, current_load):
        """Dynamically optimize batch size based on system load"""
        cpu_usage = psutil.cpu_percent(interval=0.1)
        memory_usage = psutil.virtual_memory().percent
        
        if cpu_usage > 80 or memory_usage > 85:
            return max(8, current_load // 4)  # Reduce load
        elif cpu_usage < 50 and memory_usage < 60:
            return min(128, current_load * 2)  # Increase throughput
        else:
            return current_load  # Maintain current
    
    def monitor_performance(self):
        """Monitor system performance for optimization"""
        try:
            process = psutil.Process()
            self.performance_stats.update({
                'memory_usage_mb': process.memory_info().rss / 1024 / 1024,
                'cpu_usage_percent': process.cpu_percent(),
                'cache_efficiency': self.calculate_cache_efficiency()
            })
        except:
            pass
    
    def calculate_cache_efficiency(self):
        """Calculate inference cache efficiency"""
        # Placeholder - would track cache hits/misses
        return 0.85  # 85% efficiency target

class AdvancedModelInference(ModelInference):
    def __init__(self):
        super().__init__()
        self.optimizer = ProductionInferenceOptimizer()
        self.prediction_cache_v2 = {}
        self.cache_lock = threading.RLock()
        self.inference_workers = []
        self.batch_queue = asyncio.Queue(maxsize=10000)
        
    async def load_model_production(self):
        """Production model loading with optimization"""
        await super().load_model()
        
        # Pre-warm the model with dummy data
        dummy_features = np.random.randn(1, 60, 12).astype(np.float32)
        for _ in range(10):  # Warm-up iterations
            _ = self.predict(dummy_features)
        
        # Start background batch processor
        asyncio.create_task(self.start_batch_inference_worker())
        
        logger.info("ðŸš€ Production model loaded with optimization")
    
    async def predict_production(self, features: np.ndarray, priority: str = 'NORMAL', 
                               cache_ttl: int = 30) -> Dict:
        """Production prediction with advanced caching and batching"""
        try:
            # Generate cache key
            cache_key = self.generate_cache_key_v2(features, priority)
            
            # Check cache first
            with self.cache_lock:
                if cache_key in self.prediction_cache_v2:
                    cached_result = self.prediction_cache_v2[cache_key]
                    if time.time() - cached_result['timestamp'] < cache_ttl:
                        return cached_result['result']
                    else:
                        del self.prediction_cache_v2[cache_key]
            
            # For high priority, process immediately
            if priority == 'HIGH':
                result = await self.predict_immediate(features)
            else:
                # Add to batch queue for efficient processing
                result = await self.predict_batched(features, priority)
            
            # Cache the result
            with self.cache_lock:
                self.prediction_cache_v2[cache_key] = {
                    'result': result,
                    'timestamp': time.time()
                }
                
                # Limit cache size
                if len(self.prediction_cache_v2) > 50000:
                    # Remove oldest 20% of entries
                    sorted_items = sorted(self.prediction_cache_v2.items(), 
                                        key=lambda x: x[1]['timestamp'])
                    for key, _ in sorted_items[:10000]:
                        del self.prediction_cache_v2[key]
            
            return result
            
        except Exception as e:
            logger.error(f"Production prediction error: {e}")
            return self._get_fallback_prediction()
    
    def generate_cache_key_v2(self, features: np.ndarray, priority: str) -> str:
        """Generate optimized cache key"""
        try:
            # Hash only key features to improve cache hits
            key_features = features[:, -5:, :3]  # Last 5 timesteps, first 3 features
            feature_hash = hashlib.md5(key_features.tobytes()).hexdigest()[:12]
            return f"{feature_hash}_{priority}"
        except:
            return f"fallback_{time.time()}_{priority}"
    
    async def predict_immediate(self, features: np.ndarray) -> Dict:
        """Immediate prediction for high-priority requests"""
        return self.predict(features)
    
    async def predict_batched(self, features: np.ndarray, priority: str) -> Dict:
        """Batched prediction for optimal throughput"""
        try:
            # Add to batch queue
            result_future = asyncio.Future()
            priority_value = 1 if priority == 'HIGH' else 2 if priority == 'NORMAL' else 3
            
            await self.batch_queue.put((priority_value, features, result_future))
            
            # Wait for result with timeout
            return await asyncio.wait_for(result_future, timeout=5.0)
            
        except asyncio.TimeoutError:
            logger.warning("Batch prediction timeout, falling back to immediate")
            return await self.predict_immediate(features)
    
    async def start_batch_inference_worker(self):
        """Background worker for batch inference processing"""
        while True:
            try:
                batch_items = []
                batch_features = []
                batch_futures = []
                
                # Collect batch within time window
                deadline = time.time() + 0.05  # 50ms batch window
                
                while time.time() < deadline and len(batch_items) < 32:
                    try:
                        item = await asyncio.wait_for(self.batch_queue.get(), timeout=0.01)
                        batch_items.append(item)
                        batch_features.append(item[1])
                        batch_futures.append(item[2])
                    except asyncio.TimeoutError:
                        break
                
                if batch_items:
                    # Process batch
                    try:
                        stacked_features = np.stack(batch_features)
                        batch_results = await self.process_feature_batch(stacked_features)
                        
                        # Return results to futures
                        for future, result in zip(batch_futures, batch_results):
                            if not future.done():
                                future.set_result(result)
                                
                    except Exception as e:
                        logger.error(f"Batch processing error: {e}")
                        # Return fallback results
                        for future in batch_futures:
                            if not future.done():
                                future.set_result(self._get_fallback_prediction())
                
                else:
                    await asyncio.sleep(0.001)  # Brief pause if no items
                    
            except Exception as e:
                logger.error(f"Batch worker error: {e}")
                await asyncio.sleep(0.1)
    
    async def process_feature_batch(self, batch_features: np.ndarray) -> List[Dict]:
        """Process batch of features efficiently"""
        try:
            batch_size = len(batch_features)
            results = []
            
            # Process in optimal sub-batches for TFLite
            sub_batch_size = 8  # TFLite works well with smaller batches
            
            for i in range(0, batch_size, sub_batch_size):
                sub_batch = batch_features[i:i+sub_batch_size]
                
                for features in sub_batch:
                    # Reshape for single prediction
                    single_features = features.reshape(1, *features.shape)
                    result = self.predict(single_features)
                    results.append(result)
            
            return results
            
        except Exception as e:
            logger.error(f"Batch feature processing error: {e}")
            return [self._get_fallback_prediction() for _ in range(len(batch_features))]

import psutil
import threading
from queue import PriorityQueue
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor
import resource

class ProductionInferenceOptimizer:
    def __init__(self):
        self.cpu_count = mp.cpu_count()
        self.memory_limit = self.get_memory_limit()
        self.inference_queue = PriorityQueue(maxsize=50000)
        self.batch_processor = None
        self.performance_stats = {
            'inferences_per_second': 0,
            'avg_batch_size': 0,
            'memory_usage_mb': 0,
            'cpu_usage_percent': 0,
            'cache_efficiency': 0
        }
        
    def get_memory_limit(self):
        """Get memory limit for inference caching"""
        try:
            available_memory = psutil.virtual_memory().available
            # Use 30% of available memory for model inference
            return int(available_memory * 0.3)
        except:
            return 2 * 1024 * 1024 * 1024  # 2GB default
    
    def optimize_batch_size(self, current_load):
        """Dynamically optimize batch size based on system load"""
        cpu_usage = psutil.cpu_percent(interval=0.1)
        memory_usage = psutil.virtual_memory().percent
        
        if cpu_usage > 80 or memory_usage > 85:
            return max(8, current_load // 4)  # Reduce load
        elif cpu_usage < 50 and memory_usage < 60:
            return min(128, current_load * 2)  # Increase throughput
        else:
            return current_load  # Maintain current
    
    def monitor_performance(self):
        """Monitor system performance for optimization"""
        try:
            process = psutil.Process()
            self.performance_stats.update({
                'memory_usage_mb': process.memory_info().rss / 1024 / 1024,
                'cpu_usage_percent': process.cpu_percent(),
                'cache_efficiency': self.calculate_cache_efficiency()
            })
        except:
            pass
    
    def calculate_cache_efficiency(self):
        """Calculate inference cache efficiency"""
        # Placeholder - would track cache hits/misses
        return 0.85  # 85% efficiency target

class AdvancedModelInference(ModelInference):
    def __init__(self):
        super().__init__()
        self.optimizer = ProductionInferenceOptimizer()
        self.prediction_cache_v2 = {}
        self.cache_lock = threading.RLock()
        self.inference_workers = []
        self.batch_queue = asyncio.Queue(maxsize=10000)
        
    async def load_model_production(self):
        """Production model loading with optimization"""
        await super().load_model()
        
        # Pre-warm the model with dummy data
        dummy_features = np.random.randn(1, 60, 12).astype(np.float32)
        for _ in range(10):  # Warm-up iterations
            _ = self.predict(dummy_features)
        
        # Start background batch processor
        asyncio.create_task(self.start_batch_inference_worker())
        
        logger.info("ðŸš€ Production model loaded with optimization")
    
    async def predict_production(self, features: np.ndarray, priority: str = 'NORMAL', 
                               cache_ttl: int = 30) -> Dict:
        """Production prediction with advanced caching and batching"""
        try:
            # Generate cache key
            cache_key = self.generate_cache_key_v2(features, priority)
            
            # Check cache first
            with self.cache_lock:
                if cache_key in self.prediction_cache_v2:
                    cached_result = self.prediction_cache_v2[cache_key]
                    if time.time() - cached_result['timestamp'] < cache_ttl:
                        return cached_result['result']
                    else:
                        del self.prediction_cache_v2[cache_key]
            
            # For high priority, process immediately
            if priority == 'HIGH':
                result = await self.predict_immediate(features)
            else:
                # Add to batch queue for efficient processing
                result = await self.predict_batched(features, priority)
            
            # Cache the result
            with self.cache_lock:
                self.prediction_cache_v2[cache_key] = {
                    'result': result,
                    'timestamp': time.time()
                }
                
                # Limit cache size
                if len(self.prediction_cache_v2) > 50000:
                    # Remove oldest 20% of entries
                    sorted_items = sorted(self.prediction_cache_v2.items(), 
                                        key=lambda x: x[1]['timestamp'])
                    for key, _ in sorted_items[:10000]:
                        del self.prediction_cache_v2[key]
            
            return result
            
        except Exception as e:
            logger.error(f"Production prediction error: {e}")
            return self._get_fallback_prediction()
    
    def generate_cache_key_v2(self, features: np.ndarray, priority: str) -> str:
        """Generate optimized cache key"""
        try:
            # Hash only key features to improve cache hits
            key_features = features[:, -5:, :3]  # Last 5 timesteps, first 3 features
            feature_hash = hashlib.md5(key_features.tobytes()).hexdigest()[:12]
            return f"{feature_hash}_{priority}"
        except:
            return f"fallback_{time.time()}_{priority}"
    
    async def predict_immediate(self, features: np.ndarray) -> Dict:
        """Immediate prediction for high-priority requests"""
        return self.predict(features)
    
    async def predict_batched(self, features: np.ndarray, priority: str) -> Dict:
        """Batched prediction for optimal throughput"""
        try:
            # Add to batch queue
            result_future = asyncio.Future()
            priority_value = 1 if priority == 'HIGH' else 2 if priority == 'NORMAL' else 3
            
            await self.batch_queue.put((priority_value, features, result_future))
            
            # Wait for result with timeout
            return await asyncio.wait_for(result_future, timeout=5.0)
            
        except asyncio.TimeoutError:
            logger.warning("Batch prediction timeout, falling back to immediate")
            return await self.predict_immediate(features)
    
    async def start_batch_inference_worker(self):
        """Background worker for batch inference processing"""
        while True:
            try:
                batch_items = []
                batch_features = []
                batch_futures = []
                
                # Collect batch within time window
                deadline = time.time() + 0.05  # 50ms batch window
                
                while time.time() < deadline and len(batch_items) < 32:
                    try:
                        item = await asyncio.wait_for(self.batch_queue.get(), timeout=0.01)
                        batch_items.append(item)
                        batch_features.append(item[1])
                        batch_futures.append(item[2])
                    except asyncio.TimeoutError:
                        break
                
                if batch_items:
                    # Process batch
                    try:
                        stacked_features = np.stack(batch_features)
                        batch_results = await self.process_feature_batch(stacked_features)
                        
                        # Return results to futures
                        for future, result in zip(batch_futures, batch_results):
                            if not future.done():
                                future.set_result(result)
                                
                    except Exception as e:
                        logger.error(f"Batch processing error: {e}")
                        # Return fallback results
                        for future in batch_futures:
                            if not future.done():
                                future.set_result(self._get_fallback_prediction())
                
                else:
                    await asyncio.sleep(0.001)  # Brief pause if no items
                    
            except Exception as e:
                logger.error(f"Batch worker error: {e}")
                await asyncio.sleep(0.1)
    
    async def process_feature_batch(self, batch_features: np.ndarray) -> List[Dict]:
        """Process batch of features efficiently"""
        try:
            batch_size = len(batch_features)
            results = []
            
            # Process in optimal sub-batches for TFLite
            sub_batch_size = 8  # TFLite works well with smaller batches
            
            for i in range(0, batch_size, sub_batch_size):
                sub_batch = batch_features[i:i+sub_batch_size]
                
                for features in sub_batch:
                    # Reshape for single prediction
                    single_features = features.reshape(1, *features.shape)
                    result = self.predict(single_features)
                    results.append(result)
            
            return results
            
        except Exception as e:
            logger.error(f"Batch feature processing error: {e}")
            return [self._get_fallback_prediction() for _ in range(len(batch_features))]

import tensorflow as tf
import numpy as np
import pickle
import json
import os
from typing import Dict, Optional, List
from loguru import logger
from datetime import datetime
import hashlib
import time

# Original ModelInference for compatibility
class ModelInferenceBase:
    def __init__(self):
        self.interpreter = None
        self.input_details = None
        self.output_details = None
        self.scaler = None
        self.model_metadata = None
        self.prediction_cache = {}
        
    async def load_model(self):
        try:
            self.gpu_optimizer = GPUOptimizedInference('models/momentum_model.h5')
            gpu_available = self.gpu_optimizer.initialize_gpu_inference()
            
            if gpu_available:
                self.gpu_optimizer.warm_up_gpu()
                logger.info("GPU-accelerated inference ready")
            
            self.caching_strategy = CachingStrategy()
            
            await self.load_tflite_model()
            
        except Exception as e:
            logger.error(f"Enhanced model loading failed: {e}")
            await self.load_tflite_model()
    
    async def load_tflite_model(self):
        try:
            if os.path.exists('models/model_weights.tflite'):
                logger.info("Loading production TFLite model...")
                self.interpreter = tf.lite.Interpreter(model_path='models/model_weights.tflite')
                self.interpreter.allocate_tensors()
                
                self.input_details = self.interpreter.get_input_details()
                self.output_details = self.interpreter.get_output_details()
                
                with open('models/scaler.pkl', 'rb') as f:
                    self.scaler = pickle.load(f)
                
                with open('models/model_metadata.json', 'r') as f:
                    self.model_metadata = json.load(f)
                
                logger.info(f"âœ… Production model loaded: {self.model_metadata['fingerprint']}")
                
            else:
                logger.error("âŒ NO PRODUCTION MODEL FOUND!")
                raise FileNotFoundError("Production model required - no fallbacks allowed!")
                
        except Exception as e:
            logger.error(f"âŒ CRITICAL: Model loading failed: {e}")
            raise RuntimeError("PRODUCTION MODEL REQUIRED - SYSTEM CANNOT START WITHOUT MODEL!")
    
    def predict_batch_optimized(self, features_batch: List[np.ndarray]) -> List[Dict]:
        try:
            if hasattr(self, 'gpu_optimizer') and self.gpu_optimizer.gpu_model is not None:
                stacked_features = np.stack(features_batch)
                gpu_results = self.gpu_optimizer.predict_batch_gpu(stacked_features)
                if gpu_results:
                    return gpu_results
            
            results = []
            for features in features_batch:
                result = self.predict(features)
                results.append(result)
            
            return results
            
        except Exception as e:
            logger.error(f"Batch prediction error: {e}")
            return [self._get_fallback_prediction() for _ in features_batch]
    
    def predict(self, features: np.ndarray) -> Dict:
        try:
            if self.interpreter is None:
                raise RuntimeError("PRODUCTION MODEL NOT LOADED!")
            
            
        cache_key = hash(features.tobytes())
        try:
            if hasattr(self, 'caching_strategy'):
                cache_key = self.caching_strategy.get_feature_cache_key(features)
                cached_result = self.caching_strategy.get_cached_prediction(cache_key)
                if cached_result:
                    return cached_result
            
            if self.interpreter is None:
                raise RuntimeError("PRODUCTION MODEL NOT LOADED!")
            
            if features.shape[0] == 1 and len(features.shape) == 2:
                features = features.reshape(1, -1, features.shape[-1])
            
            if features.shape[1] != self.model_metadata['sequence_length']:
                features = self.pad_or_truncate_sequence(features)
            
            features_scaled = self.scaler.transform(features.reshape(-1, features.shape[-1]))
            features_scaled = features_scaled.reshape(features.shape)
            
            self.interpreter.set_tensor(self.input_details[0]['index'], features_scaled.astype(np.float32))
            self.interpreter.invoke()
            
            momentum_score = self.interpreter.get_tensor(self.output_details[0]['index'])[0][0]
            confidence = self.interpreter.get_tensor(self.output_details[1]['index'])[0][0]
            predicted_return = self.interpreter.get_tensor(self.output_details[2]['index'])[0][0]
            predicted_volatility = self.interpreter.get_tensor(self.output_details[3]['index'])[0][0]
            
            momentum_score = self.apply_momentum_enhancement(momentum_score, features_scaled)
            confidence = self.apply_confidence_calibration(confidence, momentum_score)
            
            result = {
                'momentum_score': float(momentum_score),
                'confidence': float(confidence),
                'predicted_return': float(predicted_return),
                'predicted_volatility': float(predicted_volatility),
                'timestamp': datetime.now().isoformat(),
                'model_version': self.model_metadata['fingerprint'],
                'cache_efficiency': self.caching_strategy.get_cache_efficiency() if hasattr(self, 'caching_strategy') else 0
            }
            
            if hasattr(self, 'caching_strategy'):
                self.caching_strategy.cache_prediction(cache_key, result)
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ CRITICAL: Prediction failed: {e}")
            raise RuntimeError(f"PRODUCTION PREDICTION FAILURE: {e}")
    
    def apply_momentum_enhancement(self, momentum_score, features):
        cache_key = hash(features.tobytes())
        try:
            recent_volatility = np.std(features[0, -10:, 3])
            volume_surge = np.mean(features[0, -5:, 1]) / np.mean(features[0, -20:-5, 1])
            
            enhancement_factor = 1.0
            if recent_volatility > 0.05:
                enhancement_factor *= 1.2
            if volume_surge > 2.0:
                enhancement_factor *= 1.1
            
            enhanced_score = momentum_score * enhancement_factor
            return min(enhanced_score, 1.0)
            
        except:
            return momentum_score
    
    def apply_confidence_calibration(self, confidence, momentum_score):
        cache_key = hash(features.tobytes())
        try:
            if momentum_score > 0.8:
                confidence *= 1.1
            elif momentum_score < 0.3:
                confidence *= 0.9
            
            return min(max(confidence, 0.1), 0.95)
            
        except:
            return confidence

            if cache_key in self.prediction_cache:
                return self.prediction_cache[cache_key]
            
            # Ensure correct input shape
            if features.shape[0] == 1 and len(features.shape) == 2:
                features = features.reshape(1, -1, features.shape[-1])
            
            if features.shape[1] != self.model_metadata['sequence_length']:
                features = self.pad_or_truncate_sequence(features)
            
            # Scale features
            features_scaled = self.scaler.transform(features.reshape(-1, features.shape[-1]))
            features_scaled = features_scaled.reshape(features.shape)
            
            # Run inference
            self.interpreter.set_tensor(self.input_details[0]['index'], features_scaled.astype(np.float32))
            self.interpreter.invoke()
            
            # Get outputs
            momentum_score = self.interpreter.get_tensor(self.output_details[0]['index'])[0][0]
            confidence = self.interpreter.get_tensor(self.output_details[1]['index'])[0][0]
            predicted_return = self.interpreter.get_tensor(self.output_details[2]['index'])[0][0]
            predicted_volatility = self.interpreter.get_tensor(self.output_details[3]['index'])[0][0]
            
            result = {
                'momentum_score': float(momentum_score),
                'confidence': float(confidence),
                'predicted_return': float(predicted_return),
                'predicted_volatility': float(predicted_volatility),
                'timestamp': datetime.now().isoformat(),
                'model_version': self.model_metadata['fingerprint']
            }
            
            self.prediction_cache[cache_key] = result
            return result
            
        except Exception as e:
            logger.error(f"âŒ CRITICAL: Prediction failed: {e}")
            raise RuntimeError(f"PRODUCTION PREDICTION FAILURE: {e}")
    
    def pad_or_truncate_sequence(self, features: np.ndarray) -> np.ndarray:
        target_length = self.model_metadata['sequence_length']
        current_length = features.shape[1]
        
        if current_length > target_length:
            return features[:, -target_length:, :]
        elif current_length < target_length:
            padding = np.zeros((features.shape[0], target_length - current_length, features.shape[2]))
            return np.concatenate([padding, features], axis=1)
        else:
            return features
    
    def batch_predict(self, features_batch: List[np.ndarray]) -> List[Dict]:
        results = []
        for features in features_batch:
            result = self.predict(features)
            results.append(result)
        return results
    
    def get_model_info(self) -> Dict:
        return {
            'loaded': self.interpreter is not None,
            'metadata': self.model_metadata,
            'cache_size': len(self.prediction_cache),
            'model_type': 'PRODUCTION_TRANSFORMER'
        }

import threading
import queue
import time
from concurrent.futures import ThreadPoolExecutor
import asyncio
from dataclasses import dataclass
from typing import AsyncIterator

@dataclass
class InferenceRequest:
    features: np.ndarray
    token_address: str
    network: str
    timestamp: float
    priority: str = 'NORMAL'
    callback: callable = None

@dataclass
class BatchInferenceResult:
    predictions: List[Dict]
    batch_size: int
    inference_time: float
    model_version: str

class RealtimeInferenceEngine:
    def __init__(self, model_inference):
        self.model_inference = model_inference
        self.inference_queue = queue.PriorityQueue(maxsize=10000)
        self.batch_queue = queue.Queue(maxsize=1000)
        self.results_cache = {}
        self.inference_thread = None
        self.batch_processor = None
        self.running = False
        
        # Performance tracking
        self.inference_stats = {
            'total_predictions': 0,
            'batch_predictions': 0,
            'avg_latency_ms': 0,
            'cache_hits': 0,
            'queue_overflows': 0
        }
        
        # Batch processing settings
        self.batch_size = 32
        self.batch_timeout_ms = 50  # 50ms max wait for batch
        self.max_latency_ms = 100   # 100ms max end-to-end latency
        
    def start(self):
        """Start real-time inference engine"""
        if self.running:
            return
            
        self.running = True
        
        # Start inference threads
        self.inference_thread = threading.Thread(target=self._inference_worker, daemon=True)
        self.batch_processor = threading.Thread(target=self._batch_processor, daemon=True)
        
        self.inference_thread.start()
        self.batch_processor.start()
        
        logger.info("ðŸš€ Real-time inference engine started")
    
    def stop(self):
        """Stop real-time inference engine"""
        self.running = False
        
        if self.inference_thread:
            self.inference_thread.join(timeout=1)
        if self.batch_processor:
            self.batch_processor.join(timeout=1)
            
        logger.info("â¹ï¸ Real-time inference engine stopped")
    
    async def predict_realtime(self, features: np.ndarray, token_address: str, 
                             network: str, priority: str = 'NORMAL') -> Dict:
        """Submit prediction request for real-time processing"""
        try:
            # Check cache first
            cache_key = self._generate_cache_key(features, token_address, network)
            cached_result = self._get_cached_result(cache_key)
            
            if cached_result:
                self.inference_stats['cache_hits'] += 1
                return cached_result
            
            # Create inference request
            request = InferenceRequest(
                features=features,
                token_address=token_address,
                network=network,
                timestamp=time.time(),
                priority=priority
            )
            
            # Priority mapping for queue
            priority_values = {'HIGH': 1, 'NORMAL': 2, 'LOW': 3}
            priority_value = priority_values.get(priority, 2)
            
            try:
                # Add to queue with priority
                self.inference_queue.put((priority_value, request), timeout=0.01)
            except queue.Full:
                self.inference_stats['queue_overflows'] += 1
                # Force immediate processing for high priority
                if priority == 'HIGH':
                    return await self._process_immediate(request)
                else:
                    raise RuntimeError("Inference queue full")
            
            # Wait for result (in real implementation, would use async callback)
            result = await self._wait_for_result(cache_key, timeout=0.2)
            return result
            
        except Exception as e:
            logger.error(f"Real-time prediction error: {e}")
            return self._get_fallback_prediction()
    
    def _inference_worker(self):
        """Worker thread for processing inference requests"""
        current_batch = []
        last_batch_time = time.time()
        
        while self.running:
            try:
                # Get request with timeout
                try:
                    priority, request = self.inference_queue.get(timeout=0.01)
                    current_batch.append(request)
                except queue.Empty:
                    pass
                
                current_time = time.time()
                batch_age_ms = (current_time - last_batch_time) * 1000
                
                # Process batch if conditions met
                should_process = (
                    len(current_batch) >= self.batch_size or
                    (len(current_batch) > 0 and batch_age_ms >= self.batch_timeout_ms)
                )
                
                if should_process:
                    self._process_batch(current_batch)
                    current_batch = []
                    last_batch_time = current_time
                
            except Exception as e:
                logger.error(f"Inference worker error: {e}")
                time.sleep(0.001)
    
    def _batch_processor(self):
        """Secondary processor for batch optimization"""
        while self.running:
            try:
                # Get batched requests
                batch_requests = []
                
                # Collect requests for batch processing
                deadline = time.time() + (self.batch_timeout_ms / 1000)
                while time.time() < deadline and len(batch_requests) < self.batch_size:
                    try:
                        priority, request = self.inference_queue.get(timeout=0.001)
                        batch_requests.append(request)
                    except queue.Empty:
                        break
                
                if batch_requests:
                    self._process_batch(batch_requests)
                else:
                    time.sleep(0.001)
                    
            except Exception as e:
                logger.error(f"Batch processor error: {e}")
                time.sleep(0.001)
    
    def _process_batch(self, batch_requests: List[InferenceRequest]):
        """Process a batch of inference requests"""
        if not batch_requests:
            return
            
        start_time = time.time()
        
        try:
            # Prepare batch features
            batch_features = np.stack([req.features for req in batch_requests])
            
            # Run batch inference
            if self.model_inference.interpreter:
                batch_results = self._run_batch_inference_tflite(batch_features)
            else:
                batch_results = [self._get_fallback_prediction() for _ in batch_requests]
            
            # Cache and distribute results
            for request, result in zip(batch_requests, batch_results):
                cache_key = self._generate_cache_key(
                    request.features, request.token_address, request.network
                )
                self._cache_result(cache_key, result)
                
                # Execute callback if provided
                if request.callback:
                    try:
                        request.callback(result)
                    except Exception as e:
                        logger.error(f"Callback error: {e}")
            
            # Update statistics
            inference_time_ms = (time.time() - start_time) * 1000
            self.inference_stats['total_predictions'] += len(batch_requests)
            self.inference_stats['batch_predictions'] += 1
            
            # Update average latency
            total_latency = self.inference_stats['avg_latency_ms'] * self.inference_stats['batch_predictions']
            self.inference_stats['avg_latency_ms'] = (total_latency + inference_time_ms) / self.inference_stats['batch_predictions']
            
            logger.debug(f"Processed batch of {len(batch_requests)} in {inference_time_ms:.1f}ms")
            
        except Exception as e:
            logger.error(f"Batch processing error: {e}")
            # Return fallback results
            for request in batch_requests:
                cache_key = self._generate_cache_key(
                    request.features, request.token_address, request.network
                )
                self._cache_result(cache_key, self._get_fallback_prediction())
    
    def _run_batch_inference_tflite(self, batch_features: np.ndarray) -> List[Dict]:
        """Run batch inference using TFLite model"""
        try:
            batch_results = []
            
            # Process each sample in the batch (TFLite doesn't support batching well)
            for features in batch_features:
                if features.shape[0] == 1 and len(features.shape) == 2:
                    features = features.reshape(1, -1, features.shape[-1])
                
                if features.shape[1] != self.model_inference.model_metadata['sequence_length']:
                    features = self.model_inference.pad_or_truncate_sequence(features)
                
                # Scale features
                features_scaled = self.model_inference.scaler.transform(features.reshape(-1, features.shape[-1]))
                features_scaled = features_scaled.reshape(features.shape)
                
                # Run inference
                self.model_inference.interpreter.set_tensor(
                    self.model_inference.input_details[0]['index'], 
                    features_scaled.astype(np.float32)
                )
                self.model_inference.interpreter.invoke()
                
                # Get outputs
                momentum_score = self.model_inference.interpreter.get_tensor(
                    self.model_inference.output_details[0]['index']
                )[0][0]
                confidence = self.model_inference.interpreter.get_tensor(
                    self.model_inference.output_details[1]['index']
                )[0][0]
                predicted_return = self.model_inference.interpreter.get_tensor(
                    self.model_inference.output_details[2]['index']
                )[0][0]
                predicted_volatility = self.model_inference.interpreter.get_tensor(
                    self.model_inference.output_details[3]['index']
                )[0][0]
                
                result = {
                    'momentum_score': float(momentum_score),
                    'confidence': float(confidence),
                    'predicted_return': float(predicted_return),
                    'predicted_volatility': float(predicted_volatility),
                    'timestamp': datetime.now().isoformat(),
                    'model_version': self.model_inference.model_metadata['fingerprint'],
                    'processing_time_ms': 0  # Will be updated
                }
                
                batch_results.append(result)
            
            return batch_results
            
        except Exception as e:
            logger.error(f"TFLite batch inference error: {e}")
            return [self._get_fallback_prediction() for _ in range(len(batch_features))]
    
    async def _process_immediate(self, request: InferenceRequest) -> Dict:
        """Process high-priority request immediately"""
        try:
            result = self.model_inference.predict(request.features)
            
            # Cache the result
            cache_key = self._generate_cache_key(
                request.features, request.token_address, request.network
            )
            self._cache_result(cache_key, result)
            
            return result
            
        except Exception as e:
            logger.error(f"Immediate processing error: {e}")
            return self._get_fallback_prediction()
    
    async def _wait_for_result(self, cache_key: str, timeout: float) -> Dict:
        """Wait for result to appear in cache"""
        start_time = time.time()
        
        while time.time() - start_time < timeout:
            result = self._get_cached_result(cache_key)
            if result:
                return result
            await asyncio.sleep(0.001)  # 1ms sleep
        
        # Timeout - return fallback
        logger.warning(f"Inference timeout for key: {cache_key}")
        return self._get_fallback_prediction()
    
    def _generate_cache_key(self, features: np.ndarray, token_address: str, network: str) -> str:
        """Generate cache key for features"""
        try:
            # Use hash of features + metadata
            features_hash = hashlib.md5(features.tobytes()).hexdigest()[:16]
            return f"{token_address}_{network}_{features_hash}"
        except:
            return f"{token_address}_{network}_{int(time.time())}"
    
    def _get_cached_result(self, cache_key: str) -> Optional[Dict]:
        """Get cached result if available and fresh"""
        try:
            if cache_key in self.results_cache:
                cached_item = self.results_cache[cache_key]
                
                # Check if cache is fresh (30 seconds max age)
                if time.time() - cached_item['timestamp'] < 30:
                    return cached_item['result']
                else:
                    # Remove stale cache
                    del self.results_cache[cache_key]
                    
            return None
            
        except Exception as e:
            logger.error(f"Cache retrieval error: {e}")
            return None
    
    def _cache_result(self, cache_key: str, result: Dict):
        """Cache inference result"""
        try:
            # Limit cache size
            if len(self.results_cache) > 10000:
                # Remove oldest 20% of entries
                oldest_keys = sorted(
                    self.results_cache.keys(),
                    key=lambda k: self.results_cache[k]['timestamp']
                )[:2000]
                
                for key in oldest_keys:
                    del self.results_cache[key]
            
            self.results_cache[cache_key] = {
                'result': result,
                'timestamp': time.time()
            }
            
        except Exception as e:
            logger.error(f"Cache storage error: {e}")
    
    def _get_fallback_prediction(self) -> Dict:
        """Get fallback prediction when inference fails"""
        return {
            'momentum_score': 0.5,
            'confidence': 0.3,
            'predicted_return': 0.0,
            'predicted_volatility': 0.05,
            'timestamp': datetime.now().isoformat(),
            'model_version': 'fallback',
            'processing_time_ms': 1.0
        }
    
    def get_performance_stats(self) -> Dict:
        """Get inference engine performance statistics"""
        return {
            **self.inference_stats,
            'queue_size': self.inference_queue.qsize(),
            'cache_size': len(self.results_cache),
            'running': self.running
        }

class StreamingInference:
    def __init__(self, model_inference):
        self.model_inference = model_inference
        self.realtime_engine = RealtimeInferenceEngine(model_inference)
        self.stream_connections = {}
        
    async def start_streaming(self):
        """Start streaming inference service"""
        self.realtime_engine.start()
        logger.info("ðŸŒŠ Streaming inference started")
    
    async def stop_streaming(self):
        """Stop streaming inference service"""
        self.realtime_engine.stop()
        logger.info("â¹ï¸ Streaming inference stopped")
    
    async def create_prediction_stream(self, token_addresses: List[str], 
                                     network: str) -> AsyncIterator[Dict]:
        """Create real-time prediction stream for tokens"""
        stream_id = f"{network}_{'_'.join(token_addresses[:5])}"
        
        try:
            while True:
                predictions = []
                
                for token_address in token_addresses:
                    # Generate mock features for streaming (in real system, get from scanner)
                    features = np.random.randn(1, 60, 12)
                    
                    prediction = await self.realtime_engine.predict_realtime(
                        features, token_address, network, priority='NORMAL'
                    )
                    
                    prediction['token_address'] = token_address
                    prediction['network'] = network
                    predictions.append(prediction)
                
                yield {
                    'stream_id': stream_id,
                    'timestamp': datetime.now().isoformat(),
                    'predictions': predictions,
                    'count': len(predictions)
                }
                
                await asyncio.sleep(0.1)  # 100ms updates
                
        except Exception as e:
            logger.error(f"Streaming error: {e}")
            yield {
                'stream_id': stream_id,
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            }

# Enhanced ModelInference with real-time capabilities
class EnhancedModelInference(ModelInference):
    def __init__(self):
        super().__init__()
        self.realtime_engine = None
        self.streaming_service = None
        self.performance_monitor = {}
        
    async def load_model(self):
        try:
            self.gpu_optimizer = GPUOptimizedInference('models/momentum_model.h5')
            gpu_available = self.gpu_optimizer.initialize_gpu_inference()
            
            if gpu_available:
                self.gpu_optimizer.warm_up_gpu()
                logger.info("GPU-accelerated inference ready")
            
            self.caching_strategy = CachingStrategy()
            
            await self.load_tflite_model()
            
        except Exception as e:
            logger.error(f"Enhanced model loading failed: {e}")
            await self.load_tflite_model()
    
    async def load_tflite_model(self):
        """Enhanced model loading with real-time setup"""
        await super().load_model()
        
        # Initialize real-time components
        self.realtime_engine = RealtimeInferenceEngine(self)
        self.streaming_service = StreamingInference(self)
        
        # Start real-time engine
        await self.streaming_service.start_streaming()
        
        logger.info("âœ… Enhanced model inference loaded with real-time capabilities")
    
    async def predict_realtime(self, features: np.ndarray, token_address: str = "", 
                             network: str = "", priority: str = 'NORMAL') -> Dict:
        """Real-time prediction with sub-100ms latency"""
        if self.realtime_engine:
            return await self.realtime_engine.predict_realtime(
                features, token_address, network, priority
            )
        else:
            # Fallback to synchronous prediction
            return self.predict(features)
    
    def predict_batch_optimized(self, features_batch: List[np.ndarray]) -> List[Dict]:
        """Optimized batch prediction"""
        if not features_batch:
            return []
        
        try:
            # Stack features for efficient processing
            stacked_features = np.stack(features_batch)
            
            # Use real-time engine for batch processing
            if self.realtime_engine:
                # Process as high-priority batch
                results = []
                for features in features_batch:
                    result = asyncio.run(self.realtime_engine.predict_realtime(
                        features, "", "", "HIGH"
                    ))
                    results.append(result)
                return results
            else:
                # Fallback to individual predictions
                return [self.predict(features) for features in features_batch]
                
        except Exception as e:
            logger.error(f"Batch prediction error: {e}")
            return [self._get_fallback_prediction() for _ in features_batch]
    
    async def create_prediction_stream(self, token_addresses: List[str], 
                                     network: str) -> AsyncIterator[Dict]:
        """Create real-time prediction stream"""
        if self.streaming_service:
            async for prediction in self.streaming_service.create_prediction_stream(
                token_addresses, network
            ):
                yield prediction
        else:
            # Fallback static prediction
            yield {
                'error': 'Streaming not available',
                'timestamp': datetime.now().isoformat()
            }
    
    def get_enhanced_model_info(self) -> Dict:
        """Get enhanced model information with performance stats"""
        base_info = self.get_model_info()
        
        enhanced_info = {
            **base_info,
            'realtime_enabled': self.realtime_engine is not None,
            'streaming_enabled': self.streaming_service is not None,
        }
        
        if self.realtime_engine:
            enhanced_info['performance_stats'] = self.realtime_engine.get_performance_stats()
        
        return enhanced_info
    
    async def cleanup(self):
        """Enhanced cleanup"""
        if self.streaming_service:
            await self.streaming_service.stop_streaming()
        
        if self.realtime_engine:
            self.realtime_engine.stop()

# Replace the original ModelInference with enhanced version
ModelInference = AdvancedModelInference

logger.info("âœ… Model inference enhanced with real-time capabilities and streaming")
