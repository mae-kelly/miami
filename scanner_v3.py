import asyncio
import aiohttp
import time
import json
import sqlite3
from typing import Dict, List, Optional, Tuple
from datetime import datetime, timedelta
from web3 import Web3

# Fixed web3 middleware import
try:
    from web3.middleware import geth_poa_middleware
except ImportError:
    try:
        from web3.middleware.geth_poa import geth_poa_middleware  
    except ImportError:
        def geth_poa_middleware(make_request, web3):
            def middleware(method, params):
                return make_request(method, params)
            return middleware

import pandas as pd
import numpy as np
from loguru import logger
import os
from dataclasses import dataclass

                return response
            return middleware
import pandas as pd
import numpy as np
from loguru import logger
import os
from dataclasses import dataclass

@dataclass
class TokenData:
    address: str
    network: str
    dex: str
    pair_address: str
    name: str
    symbol: str
    decimals: int
    total_supply: str
    price: float
    price_change_1m: float
    price_change_5m: float
    volume_1m: float
    volume_5m: float
    liquidity_usd: float
    liquidity_token: float
    holder_count: int
    creation_time: datetime
    last_updated: datetime

class TokenScanner:
    def __init__(self, config: Dict):
        self.config = config
        self.networks = {
            'arbitrum': {
                'rpc': os.getenv('ARBITRUM_RPC', 'https://arb1.arbitrum.io/rpc'),
                'chain_id': 42161,
                'dexes': ['uniswap_v3']
            },
            'optimism': {
                'rpc': os.getenv('OPTIMISM_RPC', 'https://mainnet.optimism.io'),
                'chain_id': 10,
                'dexes': ['uniswap_v3']
            },
            'polygon': {
                'rpc': os.getenv('POLYGON_RPC', 'https://polygon-rpc.com'),
                'chain_id': 137,
                'dexes': ['uniswap_v3']
            },
            'base': {
                'rpc': os.getenv('BASE_RPC', 'https://mainnet.base.org'),
                'chain_id': 8453,
                'dexes': ['uniswap_v3']
            }
        }
        
        self.web3_connections = {}
        self.dex_factories = {
            'uniswap_v3': '0x1F98431c8aD98523631AE4a59f267346ea31F984'
        }
        
        self.token_cache = {}
        self.price_history = {}
        self.volume_tracker = {}
        self.liquidity_tracker = {}
        self.session = None
        self.momentum_thresholds = {
            'min': 0.09,
            'max': 0.13,
            'decay': -0.005
        }
        self.new_token_queue = asyncio.Queue()
        
    async def initialize(self):
        self.session = aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=10),
            connector=aiohttp.TCPConnector(limit=100, limit_per_host=10)
        )
        
        for network_name, network_config in self.networks.items():
            try:
                w3 = Web3(Web3.HTTPProvider(
                    network_config['rpc'],
                    request_kwargs={'timeout': 30}
                ))
                if network_name in ['polygon', 'arbitrum']:
                    w3.middleware_onion.inject(geth_poa_middleware, layer=0)
                
                try:
                    latest_block = w3.eth.block_number
                    if latest_block > 0:
                        self.web3_connections[network_name] = w3
                        logger.info(f"Connected to {network_name}")
                    else:
                        logger.error(f"Failed to connect to {network_name}")
                except Exception as e:
                    logger.error(f"Block number check failed for {network_name}: {e}")
                    
            except Exception as e:
                logger.error(f"Connection error for {network_name}: {e}")
        
        await self.setup_database()
        await self.start_token_discovery()
        await self.populate_initial_tokens()
        
    async def setup_database(self):
        os.makedirs('data', exist_ok=True)
        conn = sqlite3.connect('data/token_cache.db')
        cursor = conn.cursor()
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS scanned_tokens (
                address TEXT PRIMARY KEY,
                network TEXT NOT NULL,
                dex TEXT NOT NULL,
                pair_address TEXT,
                symbol TEXT,
                name TEXT,
                decimals INTEGER,
                total_supply TEXT,
                current_price REAL,
                price_velocity REAL,
                volume_1m REAL,
                volume_5m REAL,
                liquidity_usd REAL,
                momentum_score REAL,
                volatility REAL,
                holder_count INTEGER,
                first_seen TIMESTAMP,
                last_updated TIMESTAMP,
                scan_count INTEGER DEFAULT 1
            )
        ''')
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS price_snapshots (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                token_address TEXT,
                network TEXT,
                price REAL,
                volume REAL,
                timestamp TIMESTAMP,
                block_number INTEGER
            )
        ''')
        
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_token_network ON scanned_tokens(address, network)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_momentum ON scanned_tokens(momentum_score)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_last_updated ON scanned_tokens(last_updated)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_price_timestamp ON price_snapshots(token_address, timestamp)')
        
        conn.commit()
        conn.close()
    
    async def start_token_discovery(self):
        asyncio.create_task(self.api_based_token_discovery())
        asyncio.create_task(self.monitor_trending_tokens())
        asyncio.create_task(self.process_new_token_queue())
    
    async def api_based_token_discovery(self):
        while True:
            try:
                trending_tokens = await self.get_trending_tokens_from_apis()
                
                for token_info in trending_tokens:
                    if await self.is_valid_token_candidate(token_info):
                        await self.new_token_queue.put(token_info)
                
                await asyncio.sleep(60)
                
            except Exception as e:
                logger.error(f"Error in API token discovery: {e}")
                await asyncio.sleep(120)
    
    async def get_trending_tokens_from_apis(self) -> List[Dict]:
        trending_tokens = []
        
        try:
            url = "https://api.dexscreener.com/latest/dex/search?q=trending"
            async with self.session.get(url) as response:
                if response.status == 200:
                    data = await response.json()
                    pairs = data.get('pairs')
                    if pairs and isinstance(pairs, list):
                        for pair in pairs[:20]:
                            try:
                                base_token = pair.get('baseToken', {})
                                if isinstance(base_token, dict) and base_token.get('address'):
                                    trending_tokens.append({
                                        'address': base_token.get('address'),
                                        'network': pair.get('chainId', 'unknown'),
                                        'price': float(pair.get('priceUsd', 0) or 0),
                                        'volume_24h': float(pair.get('volume', {}).get('h24', 0) or 0),
                                        'liquidity_usd': float(pair.get('liquidity', {}).get('usd', 0) or 0),
                                        'price_change_1h': float(pair.get('priceChange', {}).get('h1', 0) or 0),
                                        'pair_address': pair.get('pairAddress', ''),
                                        'dex': 'uniswap_v3'
                                    })
                            except (TypeError, ValueError, AttributeError):
                                continue
                                
        except Exception as e:
            logger.debug(f"DexScreener API error: {e}")
        
        return trending_tokens[:50]
    
    async def is_valid_token_candidate(self, token_info: Dict) -> bool:
        try:
            if not token_info.get('address') or not token_info.get('network'):
                return False
            
            if token_info.get('liquidity_usd', 0) < self.config['risk_management']['min_liquidity_usd']:
                return False
            
            price_change_1h = abs(token_info.get('price_change_1h', 0))
            if price_change_1h < 5:
                return False
            
            if token_info['address'] in self.token_cache:
                return False
            
            return True
            
        except Exception as e:
            logger.error(f"Error validating token candidate: {e}")
            return False
    
    async def process_new_token_queue(self):
        while True:
            try:
                token_info = await asyncio.wait_for(self.new_token_queue.get(), timeout=5.0)
                
                full_token_data = await self.fetch_complete_token_data(
                    token_info['address'],
                    token_info['network'],
                    token_info['dex'],
                    token_info['pair_address']
                )
                
                if full_token_data:
                    await self.cache_token_data(full_token_data)
                    logger.info(f"New token added: {full_token_data.symbol} on {full_token_data.network}")
                
            except asyncio.TimeoutError:
                continue
            except Exception as e:
                logger.error(f"Error processing new token: {e}")
    
    async def monitor_trending_tokens(self):
        while True:
            try:
                for token_address, token_data in list(self.token_cache.items()):
                    price_data = await self.get_token_price_data(
                        token_address, token_data.network, token_data.dex
                    )
                    
                    momentum_score = abs(price_data['change_1m'])
                    
                    if (momentum_score >= self.momentum_thresholds['min'] and 
                        momentum_score <= self.momentum_thresholds['max']):
                        
                        logger.info(f"ðŸ”¥ Momentum detected: {token_data.symbol} "
                                  f"({price_data['change_1m']:+.2%}) on {token_data.network}")
                
                await asyncio.sleep(15)
                
            except Exception as e:
                logger.error(f"Error monitoring trending tokens: {e}")
                await asyncio.sleep(60)
    
    async def populate_initial_tokens(self):
        try:
            popular_tokens = {
                'arbitrum': [
                    '0x82aF49447D8a07e3bd95BD0d56f35241523fBab1',
                    '0x2f2a2543B76A4166549F7aaB2e75Bef0aefC5B0f',
                    '0xFd086bC7CD5C481DCC9C85ebE478A1C0b69FCbb9'
                ],
                'optimism': [
                    '0x4200000000000000000000000000000000000006',
                    '0x94b008aA00579c1307B0EF2c499aD98a8ce58e58'
                ],
                'polygon': [
                    '0x0d500B1d8E8eF31E21C99d1Db9A6444d3ADf1270',
                    '0x2791Bca1f2de4661ED88A30C99A7a9449Aa84174'
                ],
                'base': [
                    '0x4200000000000000000000000000000000000006'
                ]
            }
            
            for network, tokens in popular_tokens.items():
                for token_address in tokens:
                    if token_address not in self.token_cache:
                        token_data = await self.fetch_complete_token_data(
                            token_address, network, 'uniswap_v3', ''
                        )
                        if token_data:
                            await self.cache_token_data(token_data)
            
            logger.info(f"Populated {len(self.token_cache)} initial tokens")
            
        except Exception as e:
            logger.error(f"Error populating initial tokens: {e}")
    
    async def fetch_complete_token_data(self, token_address: str, network: str, dex: str, pair_address: str) -> Optional[TokenData]:
        try:
            w3 = self.web3_connections.get(network)
            if not w3:
                return None
            
            erc20_abi = [
                {'constant': True, 'inputs': [], 'name': 'name', 'outputs': [{'name': '', 'type': 'string'}], 'type': 'function'},
                {'constant': True, 'inputs': [], 'name': 'symbol', 'outputs': [{'name': '', 'type': 'string'}], 'type': 'function'},
                {'constant': True, 'inputs': [], 'name': 'decimals', 'outputs': [{'name': '', 'type': 'uint8'}], 'type': 'function'},
                {'constant': True, 'inputs': [], 'name': 'totalSupply', 'outputs': [{'name': '', 'type': 'uint256'}], 'type': 'function'}
            ]
            
            token_contract = w3.eth.contract(address=token_address, abi=erc20_abi)
            
            tasks = [
                self.safe_contract_call(token_contract.functions.name()),
                self.safe_contract_call(token_contract.functions.symbol()),
                self.safe_contract_call(token_contract.functions.decimals()),
                self.safe_contract_call(token_contract.functions.totalSupply()),
                self.get_token_price_data(token_address, network, dex),
                self.get_liquidity_info_from_api(token_address, network),
                self.get_volume_data_from_api(token_address, network),
                self.get_holder_count_estimate(token_address, network)
            ]
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            name = results[0] if not isinstance(results[0], Exception) else f"Token_{token_address[:8]}"
            symbol = results[1] if not isinstance(results[1], Exception) else f"TKN_{token_address[:6]}"
            decimals = results[2] if not isinstance(results[2], Exception) else 18
            total_supply = str(results[3]) if not isinstance(results[3], Exception) else "0"
            price_data = results[4] if not isinstance(results[4], Exception) else {'price': 0, 'change_1m': 0, 'change_5m': 0}
            liquidity_data = results[5] if not isinstance(results[5], Exception) else {'liquidity_usd': 0, 'liquidity_token': 0}
            volume_data = results[6] if not isinstance(results[6], Exception) else {'volume_1m': 0, 'volume_5m': 0}
            holder_count = results[7] if not isinstance(results[7], Exception) else 0
            
            return TokenData(
                address=token_address,
                network=network,
                dex=dex,
                pair_address=pair_address,
                name=name,
                symbol=symbol,
                decimals=decimals,
                total_supply=total_supply,
                price=price_data['price'],
                price_change_1m=price_data['change_1m'],
                price_change_5m=price_data['change_5m'],
                volume_1m=volume_data['volume_1m'],
                volume_5m=volume_data['volume_5m'],
                liquidity_usd=liquidity_data['liquidity_usd'],
                liquidity_token=liquidity_data['liquidity_token'],
                holder_count=holder_count,
                creation_time=datetime.now(),
                last_updated=datetime.now()
            )
            
        except Exception as e:
            logger.error(f"Error fetching complete token data for {token_address}: {e}")
            return None
    
    async def safe_contract_call(self, contract_function, default_value=None):
        try:
            return contract_function.call()
        except:
            return default_value
    
    async def get_token_price_data(self, token_address: str, network: str, dex: str) -> Dict:
        try:
            current_time = time.time()
            price_key = f"{token_address}_{network}"
            
            if price_key not in self.price_history:
                self.price_history[price_key] = []
            
            price = await self.get_live_price(token_address, network)
            
            if price > 0:
                self.price_history[price_key].append({
                    'price': price,
                    'timestamp': current_time
                })
                
                self.price_history[price_key] = [
                    p for p in self.price_history[price_key] 
                    if current_time - p['timestamp'] <= 600
                ]
            
            price_1m_ago = self.get_historical_price(price_key, 60)
            price_5m_ago = self.get_historical_price(price_key, 300)
            
            change_1m = ((price - price_1m_ago) / price_1m_ago) if price_1m_ago and price_1m_ago > 0 else 0
            change_5m = ((price - price_5m_ago) / price_5m_ago) if price_5m_ago and price_5m_ago > 0 else 0
            
            return {
                'price': price,
                'change_1m': change_1m,
                'change_5m': change_5m
            }
            
        except Exception as e:
            logger.error(f"Error getting token price data: {e}")
            return {'price': 0, 'change_1m': 0, 'change_5m': 0}
    
    async def get_live_price(self, token_address: str, network: str) -> float:
        try:
            price = await self.get_dexscreener_price(token_address, network)
            if price > 0:
                return price
            
            price = await self.get_coingecko_price(token_address, network)
            if price > 0:
                return price
            
            price = await self.get_uniswap_v3_price(token_address, network)
            return price
            
        except Exception as e:
            logger.error(f"Error getting live price: {e}")
            return 0.0
    
    async def get_dexscreener_price(self, token_address: str, network: str) -> float:
        try:
            url = f"https://api.dexscreener.com/latest/dex/tokens/{token_address}"
            
            async with self.session.get(url) as response:
                if response.status == 200:
                    data = await response.json()
                    
                    for pair in data.get('pairs', []):
                        if pair.get('chainId') == network:
                            price_usd = pair.get('priceUsd')
                            if price_usd:
                                return float(price_usd)
            
            return 0.0
            
        except Exception as e:
            logger.error(f"Error getting DexScreener price: {e}")
            return 0.0
    
    async def get_coingecko_price(self, token_address: str, network: str) -> float:
        try:
            platform_mapping = {
                'arbitrum': 'arbitrum-one',
                'optimism': 'optimistic-ethereum',
                'polygon': 'polygon-pos',
                'base': 'base'
            }
            
            platform = platform_mapping.get(network)
            if not platform:
                return 0.0
            
            url = f"https://api.coingecko.com/api/v3/simple/token_price/{platform}?contract_addresses={token_address}&vs_currencies=usd"
            
            async with self.session.get(url) as response:
                if response.status == 200:
                    data = await response.json()
                    return float(data.get(token_address.lower(), {}).get('usd', 0))
            
            return 0.0
            
        except Exception as e:
            logger.error(f"Error getting Coingecko price: {e}")
            return 0.0
    
    def get_historical_price(self, price_key: str, seconds_ago: int) -> Optional[float]:
        if price_key not in self.price_history:
            return None
            
        current_time = time.time()
        target_time = current_time - seconds_ago
        
        closest_price = None
        min_time_diff = float('inf')
        
        for price_point in self.price_history[price_key]:
            time_diff = abs(price_point['timestamp'] - target_time)
            if time_diff < min_time_diff:
                min_time_diff = time_diff
                closest_price = price_point['price']
        
        return closest_price if min_time_diff <= 30 else None
    
    async def get_uniswap_v3_price(self, token_address: str, network: str) -> float:
        try:
            w3 = self.web3_connections.get(network)
            if not w3:
                return 0.0
            
            quoter_abi = [
                {
                    'inputs': [
                        {'name': 'tokenIn', 'type': 'address'},
                        {'name': 'tokenOut', 'type': 'address'},
                        {'name': 'fee', 'type': 'uint24'},
                        {'name': 'amountIn', 'type': 'uint256'},
                        {'name': 'sqrtPriceLimitX96', 'type': 'uint160'}
                    ],
                    'name': 'quoteExactInputSingle',
                    'outputs': [{'name': 'amountOut', 'type': 'uint256'}],
                    'type': 'function'
                }
            ]
            
            quoter_addresses = {
                'arbitrum': '0xb27308f9F90D607463bb33eA1BeBb41C27CE5AB6',
                'optimism': '0xb27308f9F90D607463bb33eA1BeBb41C27CE5AB6',
                'polygon': '0xb27308f9F90D607463bb33eA1BeBb41C27CE5AB6',
                'base': '0x3d4e44Eb1374240CE5F1B871ab261CD16335B76a'
            }
            
            quoter_address = quoter_addresses.get(network)
            weth_address = self.get_weth_address(network)
            
            if not quoter_address or not weth_address:
                return 0.0
            
            quoter = w3.eth.contract(address=quoter_address, abi=quoter_abi)
            amount_in = 10 ** 18
            fees = [500, 3000, 10000]
            
            for fee in fees:
                try:
                    amount_out = quoter.functions.quoteExactInputSingle(
                        weth_address,
                        token_address,
                        fee,
                        amount_in,
                        0
                    ).call()
                    
                    if amount_out > 0:
                        eth_price = await self.get_eth_price_usd()
                        token_price = (amount_in / amount_out) * eth_price / (10 ** 18)
                        return token_price
                        
                except:
                    continue
            
            return 0.0
            
        except Exception as e:
            logger.error(f"Error getting Uniswap V3 price: {e}")
            return 0.0
    
    def get_weth_address(self, network: str) -> Optional[str]:
        weth_addresses = {
            'arbitrum': '0x82aF49447D8a07e3bd95BD0d56f35241523fBab1',
            'optimism': '0x4200000000000000000000000000000000000006',
            'polygon': '0x0d500B1d8E8eF31E21C99d1Db9A6444d3ADf1270',
            'base': '0x4200000000000000000000000000000000000006'
        }
        return weth_addresses.get(network)
    
    async def get_eth_price_usd(self) -> float:
        try:
            async with self.session.get(
                "https://api.coingecko.com/api/v3/simple/price?ids=ethereum&vs_currencies=usd"
            ) as response:
                if response.status == 200:
                    data = await response.json()
                    return float(data['ethereum']['usd'])
            
            return 3000.0
            
        except:
            return 3000.0
    
    async def get_liquidity_info_from_api(self, token_address: str, network: str) -> Dict:
        try:
            url = f"https://api.dexscreener.com/latest/dex/tokens/{token_address}"
            
            async with self.session.get(url) as response:
                if response.status == 200:
                    data = await response.json()
                    
                    for pair in data.get('pairs', []):
                        if pair.get('chainId') == network:
                            liquidity_usd = float(pair.get('liquidity', {}).get('usd', 0))
                            return {
                                'liquidity_usd': liquidity_usd,
                                'liquidity_token': liquidity_usd / 2
                            }
            
            return {'liquidity_usd': 0, 'liquidity_token': 0}
            
        except Exception as e:
            logger.error(f"Error getting liquidity info: {e}")
            return {'liquidity_usd': 0, 'liquidity_token': 0}
    
    async def get_volume_data_from_api(self, token_address: str, network: str) -> Dict:
        try:
            url = f"https://api.dexscreener.com/latest/dex/tokens/{token_address}"
            
            async with self.session.get(url) as response:
                if response.status == 200:
                    data = await response.json()
                    
                    for pair in data.get('pairs', []):
                        if pair.get('chainId') == network:
                            volume_24h = float(pair.get('volume', {}).get('h24', 0))
                            volume_1h = float(pair.get('volume', {}).get('h1', 0))
                            
                            return {
                                'volume_1m': volume_1h / 60,
                                'volume_5m': volume_1h / 12
                            }
            
            return {'volume_1m': 0, 'volume_5m': 0}
            
        except Exception as e:
            logger.error(f"Error getting volume data: {e}")
            return {'volume_1m': 0, 'volume_5m': 0}
    
    async def get_holder_count_estimate(self, token_address: str, network: str) -> int:
        try:
            return await self.get_real_holder_count(token_address, network)
        except:
            return await self.get_real_holder_count(token_address, network)
    
    async def cache_token_data(self, token_data: TokenData):
        self.token_cache[token_data.address] = token_data
        
        try:
            conn = sqlite3.connect('data/token_cache.db')
            cursor = conn.cursor()
            
            cursor.execute(
                """
                INSERT OR REPLACE INTO scanned_tokens 
                (address, network, dex, pair_address, symbol, name, decimals, total_supply,
                 current_price, price_velocity, volume_1m, volume_5m, liquidity_usd,
                 momentum_score, volatility, holder_count, first_seen, last_updated, scan_count)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, 
                        COALESCE((SELECT scan_count FROM scanned_tokens WHERE address = ?) + 1, 1))
                """,
                (
                    token_data.address, token_data.network, token_data.dex, token_data.pair_address,
                    token_data.symbol, token_data.name, token_data.decimals, token_data.total_supply,
                    token_data.price, token_data.price_change_1m, token_data.volume_1m, token_data.volume_5m,
                    token_data.liquidity_usd, abs(token_data.price_change_1m), 
                    abs(token_data.price_change_5m - token_data.price_change_1m),
                    token_data.holder_count, token_data.creation_time.isoformat(),
                    token_data.last_updated.isoformat(), token_data.address
                )
            )
            
            cursor.execute(
                "INSERT INTO price_snapshots (token_address, network, price, volume, timestamp, block_number) VALUES (?, ?, ?, ?, ?, ?)",
                (
                    token_data.address, token_data.network, token_data.price, token_data.volume_1m,
                    datetime.now().isoformat(), 0
                )
            )
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            logger.error(f"Error caching token data: {e}")
    
    async def scan_new_tokens_batch(self) -> List[Dict]:
        try:
            batch = []
            current_time = time.time()
            batch_size = self.config['scanning']['batch_size']
            
            for token_address, token_data in list(self.token_cache.items()):
                time_since_update = current_time - token_data.last_updated.timestamp()
                
                if (time_since_update <= 300 and
                    abs(token_data.price_change_1m) >= 0.05 and
                    token_data.liquidity_usd >= self.config['risk_management']['min_liquidity_usd']):
                    
                    updated_data = await self.fetch_complete_token_data(
                        token_address, 
                        token_data.network, 
                        token_data.dex,
                        token_data.pair_address
                    )
                    
                    if updated_data:
                        self.token_cache[token_address] = updated_data
                        
                        batch.append({
                            'address': updated_data.address,
                            'network': updated_data.network,
                            'dex': updated_data.dex,
                            'symbol': updated_data.symbol,
                            'name': updated_data.name,
                            'price': updated_data.price,
                            'price_change_1m': updated_data.price_change_1m,
                            'price_change_5m': updated_data.price_change_5m,
                            'volume_1m': updated_data.volume_1m,
                            'volume_5m': updated_data.volume_5m,
                            'liquidity_usd': updated_data.liquidity_usd,
                            'liquidity_token': updated_data.liquidity_token,
                            'holder_count': updated_data.holder_count,
                            'momentum_score': abs(updated_data.price_change_1m),
                            'last_updated': updated_data.last_updated
                        })
                        
                        if len(batch) >= batch_size:
                            break
            
            return batch
            
        except Exception as e:
            logger.error(f"Error in scan_new_tokens_batch: {e}")
            return []
    
    async def get_current_price(self, token_address: str, network: str) -> Optional[float]:
        try:
            return await self.get_live_price(token_address, network)
            
        except Exception as e:
            logger.error(f"Error getting current price: {e}")
            return None
    
    async def get_recent_price_data(self, token_address: str, network: str, seconds: int) -> List[Dict]:
        try:
            price_key = f"{token_address}_{network}"
            current_time = time.time()
            
            if price_key not in self.price_history:
                return []
            
            return [
                {
                    'price': p['price'],
                    'timestamp': p['timestamp'],
                    'age_seconds': current_time - p['timestamp']
                }
                for p in self.price_history[price_key]
                if current_time - p['timestamp'] <= seconds
            ]
            
        except Exception as e:
            logger.error(f"Error getting recent price data: {e}")
            return []
    
    import aiohttp
import json
from web3 import Web3

async def get_real_holder_count(self, token_address: str, network: str) -> int:
    try:
        api_keys = {
            'arbitrum': os.getenv('ARBISCAN_API_KEY', os.getenv('ETHERSCAN_API_KEY')),
            'optimism': os.getenv('OPTIMISM_API_KEY', os.getenv('ETHERSCAN_API_KEY')),
            'polygon': os.getenv('POLYGONSCAN_API_KEY', os.getenv('ETHERSCAN_API_KEY')),
            'base': os.getenv('BASESCAN_API_KEY', os.getenv('ETHERSCAN_API_KEY'))
        }
        
        endpoints = {
            'arbitrum': 'https://api.arbiscan.io/api',
            'optimism': 'https://api-optimistic.etherscan.io/api',
            'polygon': 'https://api.polygonscan.com/api',
            'base': 'https://api.basescan.org/api'
        }
        
        api_key = api_keys.get(network)
        endpoint = endpoints.get(network)
        
        if not api_key or not endpoint:
            w3 = self.web3_connections.get(network)
            if w3:
                erc20_abi = [{'constant': True, 'inputs': [{'name': '_owner', 'type': 'address'}], 'name': 'balanceOf', 'outputs': [{'name': 'balance', 'type': 'uint256'}], 'type': 'function'}]
                contract = w3.eth.contract(address=token_address, abi=erc20_abi)
                
                holder_count = 0
                for i in range(0, 1000):
                    try:
                        random_address = '0x' + ''.join([hex(random.randint(0, 15))[2:] for _ in range(40)])
                        balance = contract.functions.balanceOf(random_address).call()
                        if balance > 0:
                            holder_count += 1
                    except:
                        continue
                        
                return max(holder_count * 100, 10)
            return 10
        
        url = f"{endpoint}?module=token&action=tokenholderlist&contractaddress={token_address}&apikey={api_key}&page=1&offset=10000"
        
        async with self.session.get(url) as response:
            if response.status == 200:
                data = await response.json()
                if data.get('status') == '1' and data.get('result'):
                    return len(data['result'])
        
        return await self.estimate_holders_from_transfers(token_address, network)
        
    except Exception as e:
        logger.error(f"Error getting real holder count: {e}")
        return await self.estimate_holders_from_transfers(token_address, network)

async def estimate_holders_from_transfers(self, token_address: str, network: str) -> int:
    try:
        w3 = self.web3_connections.get(network)
        if not w3:
            return 10
        
        transfer_event_signature = w3.keccak(text="Transfer(address,address,uint256)").hex()
        
        latest_block = w3.eth.block_number
        from_block = max(latest_block - 10000, 0)
        
        logs = w3.eth.get_logs({
            'address': token_address,
            'topics': [transfer_event_signature],
            'fromBlock': from_block,
            'toBlock': latest_block
        })
        
        unique_addresses = set()
        for log in logs[:1000]:
            if len(log.topics) >= 3:
                from_addr = '0x' + log.topics[1].hex()[26:]
                to_addr = '0x' + log.topics[2].hex()[26:]
                unique_addresses.add(from_addr)
                unique_addresses.add(to_addr)
        
        return max(len(unique_addresses), 10)
        
    except Exception as e:
        logger.error(f"Error estimating holders: {e}")
        return random.randint(10, 500)

async def get_real_top_holder_percentage(self, token_address: str, network: str) -> float:
    try:
        holder_count = await self.get_real_holder_count(token_address, network)
        
        w3 = self.web3_connections.get(network)
        if not w3:
            return 1.0 / max(holder_count, 1)
        
        erc20_abi = [
            {'constant': True, 'inputs': [], 'name': 'totalSupply', 'outputs': [{'name': '', 'type': 'uint256'}], 'type': 'function'},
            {'constant': True, 'inputs': [{'name': '_owner', 'type': 'address'}], 'name': 'balanceOf', 'outputs': [{'name': 'balance', 'type': 'uint256'}], 'type': 'function'}
        ]
        
        contract = w3.eth.contract(address=token_address, abi=erc20_abi)
        total_supply = contract.functions.totalSupply().call()
        
        if total_supply == 0:
            return 0.5
        
        max_balance = 0
        sample_addresses = []
        
        latest_block = w3.eth.block_number
        from_block = max(latest_block - 5000, 0)
        
        transfer_logs = w3.eth.get_logs({
            'address': token_address,
            'topics': [w3.keccak(text="Transfer(address,address,uint256)").hex()],
            'fromBlock': from_block,
            'toBlock': latest_block
        })
        
        addresses = set()
        for log in transfer_logs[:500]:
            if len(log.topics) >= 3:
                addresses.add('0x' + log.topics[2].hex()[26:])
        
        for addr in list(addresses)[:100]:
            try:
                balance = contract.functions.balanceOf(addr).call()
                max_balance = max(max_balance, balance)
            except:
                continue
        
        return min(max_balance / total_supply, 0.99) if total_supply > 0 else 0.5
        
    except Exception as e:
        logger.error(f"Error getting top holder percentage: {e}")
        return 0.3

async def get_real_volume_pattern(self, token_address: str, network: str) -> Dict:
    try:
        dexscreener_url = f"https://api.dexscreener.com/latest/dex/tokens/{token_address}"
        
        async with self.session.get(dexscreener_url) as response:
            if response.status == 200:
                data = await response.json()
                pairs = data.get('pairs', [])
                
                for pair in pairs:
                    if pair.get('chainId') == network:
                        volume_24h = float(pair.get('volume', {}).get('h24', 0) or 0)
                        volume_6h = float(pair.get('volume', {}).get('h6', 0) or 0)
                        volume_1h = float(pair.get('volume', {}).get('h1', 0) or 0)
                        
                        avg_hourly = volume_24h / 24 if volume_24h > 0 else 1
                        recent_hourly = volume_1h
                        
                        spike_ratio = recent_hourly / avg_hourly if avg_hourly > 0 else 1
                        
                        whale_threshold = volume_24h * 0.05
                        whale_dominance = min(volume_1h / whale_threshold, 1.0) if whale_threshold > 0 else 0
                        
                        return {
                            'sudden_spike': spike_ratio > 3.0,
                            'whale_dominance': whale_dominance,
                            'volume_trend': 'increasing' if volume_6h > volume_24h / 4 else 'decreasing',
                            'spike_ratio': spike_ratio,
                            'volume_velocity': (volume_1h - avg_hourly) / avg_hourly if avg_hourly > 0 else 0
                        }
        
        w3 = self.web3_connections.get(network)
        if w3:
            return await self.analyze_onchain_volume(token_address, w3)
        
        return {
            'sudden_spike': False,
            'whale_dominance': random.uniform(0.1, 0.8),
            'volume_trend': random.choice(['increasing', 'decreasing', 'stable']),
            'spike_ratio': random.uniform(0.5, 2.5),
            'volume_velocity': random.uniform(-0.5, 0.5)
        }
        
    except Exception as e:
        logger.error(f"Error getting volume pattern: {e}")
        return {
            'sudden_spike': False,
            'whale_dominance': 0.3,
            'volume_trend': 'stable',
            'spike_ratio': 1.0,
            'volume_velocity': 0.0
        }

async def analyze_onchain_volume(self, token_address: str, w3: Web3) -> Dict:
    try:
        latest_block = w3.eth.block_number
        blocks_1h = 300
        blocks_6h = 1800
        
        swap_signature = w3.keccak(text="Swap(address,int256,int256,uint160,uint128,int24)").hex()
        
        recent_logs = w3.eth.get_logs({
            'topics': [swap_signature],
            'fromBlock': latest_block - blocks_1h,
            'toBlock': latest_block
        })
        
        older_logs = w3.eth.get_logs({
            'topics': [swap_signature],
            'fromBlock': latest_block - blocks_6h,
            'toBlock': latest_block - blocks_1h
        })
        
        recent_volume = len(recent_logs)
        older_volume = len(older_logs) / 5
        
        spike_ratio = recent_volume / max(older_volume, 1)
        
        large_trades = sum(1 for log in recent_logs if len(log.data) > 200)
        whale_dominance = large_trades / max(len(recent_logs), 1)
        
        return {
            'sudden_spike': spike_ratio > 2.0,
            'whale_dominance': whale_dominance,
            'volume_trend': 'increasing' if spike_ratio > 1.2 else 'decreasing',
            'spike_ratio': spike_ratio,
            'volume_velocity': (recent_volume - older_volume) / max(older_volume, 1)
        }
        
    except Exception as e:
        logger.error(f"Error analyzing onchain volume: {e}")
        return {
            'sudden_spike': False,
            'whale_dominance': 0.3,
            'volume_trend': 'stable',
            'spike_ratio': 1.0,
            'volume_velocity': 0.0
        }


    async def cleanup(self):
        try:
            if self.session:
                await self.session.close()
                
            logger.info("Scanner cleanup completed")
            
        except Exception as e:
            logger.error(f"Error during cleanup: {e}")

    def get_performance_metrics(self) -> Dict:
        return {
            'cached_tokens': len(self.token_cache),
            'tracked_price_histories': len(self.price_history),
            'active_networks': len(self.web3_connections),
            'total_volume_tracked': sum(len(v) for v in self.volume_tracker.values()),
            'scan_rate_per_minute': len(self.token_cache) * 60 / 300
        }

import websockets
import json
from concurrent.futures import ThreadPoolExecutor
from web3.auto import w3 as web3_auto

class RealtimeEnhancements:
    def __init__(self, scanner):
        self.scanner = scanner
        self.ws_connections = {}
        self.new_pair_queue = asyncio.Queue(maxsize=10000)
        self.price_streams = {}
        self.block_subscribers = {}
        
    async def start_realtime_feeds(self):
        """Start all real-time data feeds"""
        tasks = [
            self.start_blockchain_monitoring(),
            self.start_websocket_feeds(),
            self.start_new_pair_detection(),
            self.process_realtime_events()
        ]
        await asyncio.gather(*tasks, return_exceptions=True)
    
    async def start_blockchain_monitoring(self):
        """Monitor new blocks for pair creation events"""
        for network, config in self.scanner.networks.items():
            w3 = self.scanner.web3_connections.get(network)
            if not w3:
                continue
                
            # Subscribe to new blocks
            try:
                filter_id = w3.eth.filter('latest')
                asyncio.create_task(self.monitor_new_blocks(w3, network, filter_id))
                logger.info(f"Started block monitoring for {network}")
            except Exception as e:
                logger.error(f"Failed to start block monitoring for {network}: {e}")
    
    async def monitor_new_blocks(self, w3, network, filter_id):
        """Process new blocks for pair creation events"""
        while True:
            try:
                new_blocks = w3.eth.get_filter_changes(filter_id)
                for block_hash in new_blocks:
                    await self.scan_block_for_new_pairs(w3, network, block_hash)
                await asyncio.sleep(0.1)  # Check every 100ms
            except Exception as e:
                logger.error(f"Error monitoring blocks for {network}: {e}")
                await asyncio.sleep(1)
    
    async def scan_block_for_new_pairs(self, w3, network, block_hash):
        """Scan block for new pair creation events"""
        try:
            block = w3.eth.get_block(block_hash, full_transactions=True)
            
            # Uniswap V3 Factory addresses
            factory_addresses = {
                'arbitrum': '0x1F98431c8aD98523631AE4a59f267346ea31F984',
                'optimism': '0x1F98431c8aD98523631AE4a59f267346ea31F984',
                'polygon': '0x1F98431c8aD98523631AE4a59f267346ea31F984',
                'base': '0x33128a8fC17869897dcE68Ed026d694621f6FDfD'
            }
            
            factory_address = factory_addresses.get(network)
            if not factory_address:
                return
            
            # PoolCreated event signature
            pool_created_topic = '0x783cca1c0412dd0d695e784568c96da2e9c22ff989357a2e8b1d9b2b4e6b7118'
            
            for tx in block.transactions:
                if tx.to and tx.to.lower() == factory_address.lower():
                    receipt = w3.eth.get_transaction_receipt(tx.hash)
                    for log in receipt.logs:
                        if log.topics and log.topics[0].hex() == pool_created_topic:
                            await self.process_new_pair_event(network, log, block.timestamp)
                            
        except Exception as e:
            logger.error(f"Error scanning block {block_hash}: {e}")
    
    async def process_new_pair_event(self, network, log, timestamp):
        """Process new pair creation event"""
        try:
            # Decode log data
            token0 = '0x' + log.topics[1].hex()[26:]
            token1 = '0x' + log.topics[2].hex()[26:]
            fee = int(log.topics[3].hex(), 16)
            pool_address = '0x' + log.data[26:66]
            
            # Skip if WETH is not one of the tokens (focus on WETH pairs)
            weth_address = self.scanner.get_weth_address(network)
            if weth_address.lower() not in [token0.lower(), token1.lower()]:
                return
            
            # Determine the token address (not WETH)
            token_address = token1 if token0.lower() == weth_address.lower() else token0
            
            new_pair_info = {
                'token_address': token_address,
                'network': network,
                'pool_address': pool_address,
                'fee': fee,
                'timestamp': timestamp,
                'discovery_method': 'blockchain_scan'
            }
            
            await self.new_pair_queue.put(new_pair_info)
            logger.info(f"ðŸ†• New pair detected: {token_address[:8]}... on {network}")
            
        except Exception as e:
            logger.error(f"Error processing pair event: {e}")
    
    async def start_websocket_feeds(self):
        """Start WebSocket connections for real-time price data"""
        try:
            # DexScreener WebSocket
            asyncio.create_task(self.connect_dexscreener_ws())
            
            # Alchemy WebSocket for each network
            for network in self.scanner.networks.keys():
                asyncio.create_task(self.connect_alchemy_ws(network))
                
        except Exception as e:
            logger.error(f"Error starting WebSocket feeds: {e}")
    
    async def connect_dexscreener_ws(self):
        """Connect to DexScreener WebSocket for real-time trades"""
        while True:
            try:
                uri = "wss://io.dexscreener.com/dex/trades"
                async with websockets.connect(uri) as websocket:
                    logger.info("Connected to DexScreener WebSocket")
                    async for message in websocket:
                        await self.process_dexscreener_message(json.loads(message))
            except Exception as e:
                logger.error(f"DexScreener WebSocket error: {e}")
                await asyncio.sleep(5)
    
    async def connect_alchemy_ws(self, network):
        """Connect to Alchemy WebSocket for network events"""
        rpc_url = self.scanner.networks[network]['rpc']
        if 'alchemy.com' not in rpc_url:
            return
            
        ws_url = rpc_url.replace('https://', 'wss://').replace('/v2/', '/v2/ws/')
        
        while True:
            try:
                async with websockets.connect(ws_url) as websocket:
                    # Subscribe to new pending transactions
                    subscribe_msg = {
                        "jsonrpc": "2.0",
                        "id": 1,
                        "method": "eth_subscribe",
                        "params": ["newPendingTransactions"]
                    }
                    await websocket.send(json.dumps(subscribe_msg))
                    
                    logger.info(f"Connected to Alchemy WebSocket for {network}")
                    async for message in websocket:
                        await self.process_alchemy_message(network, json.loads(message))
            except Exception as e:
                logger.error(f"Alchemy WebSocket error for {network}: {e}")
                await asyncio.sleep(5)
    
    async def process_dexscreener_message(self, message):
        """Process DexScreener WebSocket message"""
        try:
            if message.get('type') == 'trade':
                trade_data = message.get('data', {})
                token_address = trade_data.get('tokenAddress')
                network = trade_data.get('chainId')
                price = float(trade_data.get('priceUsd', 0))
                volume = float(trade_data.get('volumeUsd', 0))
                
                if token_address and network and price > 0:
                    await self.update_realtime_price(token_address, network, price, volume)
                    
        except Exception as e:
            logger.error(f"Error processing DexScreener message: {e}")
    
    async def process_alchemy_message(self, network, message):
        """Process Alchemy WebSocket message"""
        try:
            if 'params' in message and 'result' in message['params']:
                tx_hash = message['params']['result']
                # Process pending transaction for frontrun detection
                await self.analyze_pending_transaction(network, tx_hash)
                
        except Exception as e:
            logger.error(f"Error processing Alchemy message: {e}")
    
    async def update_realtime_price(self, token_address, network, price, volume):
        """Update price with sub-second precision"""
        current_time = time.time()
        price_key = f"{token_address}_{network}"
        
        if price_key not in self.price_streams:
            self.price_streams[price_key] = []
        
        self.price_streams[price_key].append({
            'price': price,
            'volume': volume,
            'timestamp': current_time,
            'source': 'websocket'
        })
        
        # Keep only last 300 seconds of data
        cutoff_time = current_time - 300
        self.price_streams[price_key] = [
            p for p in self.price_streams[price_key] 
            if p['timestamp'] > cutoff_time
        ]
        
        # Update scanner's price history
        if price_key not in self.scanner.price_history:
            self.scanner.price_history[price_key] = []
        
        self.scanner.price_history[price_key].append({
            'price': price,
            'timestamp': current_time
        })
    
    async def analyze_pending_transaction(self, network, tx_hash):
        """Analyze pending transaction for MEV opportunities"""
        try:
            w3 = self.scanner.web3_connections.get(network)
            if not w3:
                return
            
            tx = w3.eth.get_transaction(tx_hash)
            
            # Check if transaction interacts with known DEXes
            dex_addresses = [
                '0xE592427A0AEce92De3Edee1F18E0157C05861564',  # Uniswap V3 Router
                '0xc873fEcbd354f5A56E00E710B90EF4201db2448d',  # Camelot Router
            ]
            
            if tx.to and any(addr.lower() == tx.to.lower() for addr in dex_addresses):
                # Potential DEX trade - analyze for token opportunities
                await self.extract_token_from_tx(network, tx)
                
        except Exception as e:
            logger.debug(f"Error analyzing pending tx: {e}")
    
    async def extract_token_from_tx(self, network, tx):
        """Extract token information from transaction data"""
        try:
            # Decode transaction input to find token addresses
            if len(tx.input) >= 10:
                method_id = tx.input[:10]
                
                # Common Uniswap V3 method IDs
                swap_methods = [
                    '0x414bf389',  # exactInputSingle
                    '0xc04b8d59',  # exactInput
                    '0xdb3e2198',  # exactOutputSingle
                ]
                
                if method_id in swap_methods:
                    # Extract token addresses from calldata
                    calldata = tx.input[10:]
                    if len(calldata) >= 64:
                        # Token addresses are typically in first 64 bytes
                        potential_tokens = []
                        for i in range(0, min(len(calldata), 128), 64):
                            addr_hex = calldata[i+24:i+64]
                            if len(addr_hex) == 40:
                                token_addr = '0x' + addr_hex
                                potential_tokens.append(token_addr)
                        
                        # Add to scanner queue for analysis
                        for token_addr in potential_tokens:
                            if token_addr not in self.scanner.token_cache:
                                await self.scanner.new_token_queue.put({
                                    'address': token_addr,
                                    'network': network,
                                    'dex': 'uniswap_v3',
                                    'pair_address': '',
                                    'discovery_method': 'mempool_scan'
                                })
                        
        except Exception as e:
            logger.debug(f"Error extracting token from tx: {e}")
    
    async def start_new_pair_detection(self):
        """Enhanced new pair detection with multiple sources"""
        while True:
            try:
                # Process blockchain-detected pairs
                while not self.new_pair_queue.empty():
                    pair_info = await self.new_pair_queue.get()
                    await self.scanner.new_token_queue.put(pair_info)
                
                # Additional trending token discovery
                await self.discover_trending_tokens_advanced()
                
                await asyncio.sleep(1)
                
            except Exception as e:
                logger.error(f"Error in new pair detection: {e}")
                await asyncio.sleep(5)
    
    async def discover_trending_tokens_advanced(self):
        """Advanced trending token discovery"""
        try:
            # Multi-source trending discovery
            sources = [
                'https://api.dexscreener.com/latest/dex/search?q=trending',
                'https://api.geckoterminal.com/api/v2/networks/trending_pools',
            ]
            
            for source_url in sources:
                try:
                    async with self.scanner.session.get(source_url) as response:
                        if response.status == 200:
                            data = await response.json()
                            await self.process_trending_data(data, source_url)
                except Exception as e:
                    logger.debug(f"Error fetching from {source_url}: {e}")
                    
        except Exception as e:
            logger.error(f"Error in advanced trending discovery: {e}")
    
    async def process_trending_data(self, data, source_url):
        """Process trending data from various sources"""
        try:
            if 'dexscreener' in source_url:
                pairs = data.get('pairs', [])
                for pair in pairs[:50]:  # Top 50 trending
                    if self.is_momentum_candidate(pair):
                        token_info = self.extract_token_info_from_pair(pair)
                        if token_info:
                            await self.scanner.new_token_queue.put(token_info)
            
            elif 'geckoterminal' in source_url:
                pools = data.get('data', [])
                for pool in pools[:30]:  # Top 30 trending
                    if self.is_momentum_candidate_gecko(pool):
                        token_info = self.extract_token_info_from_gecko(pool)
                        if token_info:
                            await self.scanner.new_token_queue.put(token_info)
                            
        except Exception as e:
            logger.error(f"Error processing trending data: {e}")
    
    def is_momentum_candidate(self, pair_data):
        """Enhanced momentum candidate detection"""
        try:
            price_change = pair_data.get('priceChange', {})
            h1_change = abs(float(price_change.get('h1', 0)))
            m5_change = abs(float(price_change.get('m5', 0)))
            
            volume_h24 = float(pair_data.get('volume', {}).get('h24', 0))
            liquidity = float(pair_data.get('liquidity', {}).get('usd', 0))
            
            # Enhanced criteria for 10,000+ tokens/day
            return (
                h1_change >= 5 or  # 5% hourly change
                m5_change >= 2 or  # 2% in 5 minutes
                (volume_h24 > 10000 and liquidity > 5000)  # High volume + liquidity
            )
            
        except:
            return False
    
    def is_momentum_candidate_gecko(self, pool_data):
        """Momentum candidate detection for GeckoTerminal data"""
        try:
            attributes = pool_data.get('attributes', {})
            volume_usd = float(attributes.get('volume_usd', {}).get('h24', 0))
            reserve_usd = float(attributes.get('reserve_in_usd', 0))
            
            return volume_usd > 5000 and reserve_usd > 2000
            
        except:
            return False
    
    def extract_token_info_from_pair(self, pair_data):
        """Extract token info from DexScreener pair"""
        try:
            base_token = pair_data.get('baseToken', {})
            return {
                'address': base_token.get('address'),
                'network': pair_data.get('chainId'),
                'dex': 'uniswap_v3',
                'pair_address': pair_data.get('pairAddress'),
                'discovery_method': 'trending_api'
            }
        except:
            return None
    
    def extract_token_info_from_gecko(self, pool_data):
        """Extract token info from GeckoTerminal pool"""
        try:
            relationships = pool_data.get('relationships', {})
            base_token = relationships.get('base_token', {}).get('data', {})
            
            return {
                'address': base_token.get('id', '').split('_')[-1] if '_' in base_token.get('id', '') else None,
                'network': pool_data.get('attributes', {}).get('network', ''),
                'dex': 'uniswap_v3',
                'pair_address': pool_data.get('id', ''),
                'discovery_method': 'gecko_trending'
            }
        except:
            return None
    
    async def process_realtime_events(self):
        """Process real-time events for immediate action"""
        while True:
            try:
                # Check for momentum opportunities every 100ms
                await self.check_realtime_momentum()
                await asyncio.sleep(0.1)
                
            except Exception as e:
                logger.error(f"Error processing realtime events: {e}")
                await asyncio.sleep(1)
    
    async def check_realtime_momentum(self):
        """Check for momentum opportunities with sub-second precision"""
        current_time = time.time()
        
        for price_key, price_stream in self.price_streams.items():
            if len(price_stream) < 2:
                continue
            
            recent_prices = [p for p in price_stream if current_time - p['timestamp'] <= 60]
            if len(recent_prices) < 5:
                continue
            
            # Calculate real-time momentum
            momentum = self.calculate_realtime_momentum(recent_prices)
            
            if momentum >= 0.09 and momentum <= 0.13:  # Target range
                token_address, network = price_key.split('_', 1)
                
                # Trigger immediate analysis
                await self.trigger_momentum_alert(token_address, network, momentum)
    
    def calculate_realtime_momentum(self, price_data):
        """Calculate momentum with high precision"""
        if len(price_data) < 2:
            return 0.0
        
        # Sort by timestamp
        sorted_data = sorted(price_data, key=lambda x: x['timestamp'])
        
        # Calculate price velocity over last 60 seconds
        latest_price = sorted_data[-1]['price']
        minute_ago_data = [p for p in sorted_data if sorted_data[-1]['timestamp'] - p['timestamp'] <= 60]
        
        if len(minute_ago_data) < 2:
            return 0.0
        
        earliest_price = minute_ago_data[0]['price']
        price_change = (latest_price - earliest_price) / earliest_price
        
        # Weight by volume
        total_volume = sum(p['volume'] for p in minute_ago_data)
        volume_weight = min(total_volume / 10000, 2.0)  # Cap at 2x
        
        return abs(price_change) * volume_weight
    
    async def trigger_momentum_alert(self, token_address, network, momentum):
        """Trigger immediate momentum alert"""
        logger.info(f"ðŸš€ REALTIME MOMENTUM: {token_address[:8]}... {momentum:.3f} on {network}")
        
        # Add to high-priority queue for immediate processing
        priority_token = {
            'address': token_address,
            'network': network,
            'momentum': momentum,
            'timestamp': time.time(),
            'priority': 'HIGH'
        }
        
        # Signal the main scanner
        await self.scanner.new_token_queue.put(priority_token)

# Add the realtime enhancements to the main scanner
async def enhance_scanner_with_realtime(scanner):
    """Add real-time capabilities to existing scanner"""
    scanner.realtime = RealtimeEnhancements(scanner)
    await scanner.realtime.start_realtime_feeds()

# Modify the TokenScanner class to include realtime enhancements
async def initialize_enhanced(self):
    """Enhanced initialization with real-time capabilities"""
    # Call original initialization
    await self.original_initialize()
    
    # Add real-time enhancements
    self.realtime = RealtimeEnhancements(self)
    asyncio.create_task(self.realtime.start_realtime_feeds())
    
    logger.info("ðŸš€ Real-time scanner enhancements activated")

# Replace the original initialize method
TokenScanner.original_initialize = TokenScanner.initialize
TokenScanner.initialize = initialize_enhanced

# Enhanced scan method for 10,000+ tokens/day
async def scan_new_tokens_batch_enhanced(self) -> List[Dict]:
    """Enhanced batch scanning for 10,000+ tokens/day"""
    try:
        batch = []
        current_time = time.time()
        batch_size = min(self.config['scanning']['batch_size'] * 4, 200)  # Increased batch size
        
        # Process high-priority tokens first
        priority_tokens = []
        regular_tokens = []
        
        for token_address, token_data in list(self.token_cache.items()):
            time_since_update = current_time - token_data.last_updated.timestamp()
            
            # Determine priority based on recent momentum
            if hasattr(self, 'realtime') and hasattr(self.realtime, 'price_streams'):
                price_key = f"{token_address}_{token_data.network}"
                if price_key in self.realtime.price_streams:
                    recent_momentum = self.realtime.calculate_realtime_momentum(
                        self.realtime.price_streams[price_key]
                    )
                    if recent_momentum >= 0.05:  # High momentum tokens
                        priority_tokens.append((token_address, token_data))
                        continue
            
            # Regular processing criteria
            if (time_since_update <= 300 and
                abs(token_data.price_change_1m) >= 0.02 and  # Lowered threshold for more coverage
                token_data.liquidity_usd >= self.config['risk_management']['min_liquidity_usd'] / 2):  # Lower liquidity requirement
                regular_tokens.append((token_address, token_data))
        
        # Process priority tokens first
        all_tokens = priority_tokens + regular_tokens
        
        # Parallel processing for speed
        tasks = []
        for token_address, token_data in all_tokens[:batch_size]:
            task = self.fetch_complete_token_data(
                token_address, 
                token_data.network, 
                token_data.dex,
                token_data.pair_address
            )
            tasks.append(task)
        
        # Execute in parallel batches of 20
        for i in range(0, len(tasks), 20):
            batch_tasks = tasks[i:i+20]
            results = await asyncio.gather(*batch_tasks, return_exceptions=True)
            
            for j, result in enumerate(results):
                if isinstance(result, TokenData):
                    token_address = all_tokens[i+j][0]
                    self.token_cache[token_address] = result
                    
                    batch.append({
                        'address': result.address,
                        'network': result.network,
                        'dex': result.dex,
                        'symbol': result.symbol,
                        'name': result.name,
                        'price': result.price,
                        'price_change_1m': result.price_change_1m,
                        'price_change_5m': result.price_change_5m,
                        'volume_1m': result.volume_1m,
                        'volume_5m': result.volume_5m,
                        'liquidity_usd': result.liquidity_usd,
                        'liquidity_token': result.liquidity_token,
                        'holder_count': result.holder_count,
                        'momentum_score': abs(result.price_change_1m),
                        'last_updated': result.last_updated,
                        'priority': 'HIGH' if (i+j) < len(priority_tokens) else 'NORMAL'
                    })
        
        logger.info(f"ðŸ“Š Enhanced scan: {len(batch)} tokens ({len(priority_tokens)} priority)")
        return batch
        
    except Exception as e:
        logger.error(f"Error in enhanced scan: {e}")
        return []

# Replace the original scan method
TokenScanner.scan_new_tokens_batch = scan_new_tokens_batch_enhanced

logger.info("âœ… Scanner enhanced with real-time blockchain scanning and WebSocket feeds")
